{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# \"patient_id exam_id spot[1-14] frame_number score\"\n",
    "# load patients data in a dataframe from images folder\n",
    "import glob\n",
    "images_paths = glob.glob(\"images/*.png\", recursive=True)\n",
    "# images are named as: patientid_examid_spotnumber_framenumber_score.png\n",
    "# create a dataframe with the data removing \"images/\"\n",
    "images_df = pd.DataFrame([path[7:-4].split(\"_\") for path in images_paths], columns=[\"patient_id\", \"exam_id\", \"spot\", \"frame_number\", \"score\"])\n",
    "images_df[\"score\"] = images_df[\"score\"].astype(str)\n",
    "images_df[\"frame_number\"] = images_df[\"frame_number\"].astype(str)\n",
    "images_df[\"spot\"] = images_df[\"spot\"].astype(str)\n",
    "images_df[\"patient_id\"] = images_df[\"patient_id\"].astype(str)\n",
    "images_df[\"exam_id\"] = images_df[\"exam_id\"].astype(str)\n",
    "\n",
    "# save images_df to excel\n",
    "images_df.to_excel(\"images_df.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Davide\\Documents\\MID\\project.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Davide/Documents/MID/project.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m e \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1127\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Davide/Documents/MID/project.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Davide/Documents/MID/project.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(images_df[(images_df[\u001b[39m\"\u001b[39m\u001b[39mpatient_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m p) \u001b[39m&\u001b[39m (images_df[\u001b[39m\"\u001b[39m\u001b[39mexam_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m e \u001b[39mor\u001b[39;00m e \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m&\u001b[39m (images_df[\u001b[39m\"\u001b[39m\u001b[39mspot\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m s)]\u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcount())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Davide/Documents/MID/project.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Davide/Documents/MID/project.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m random\u001b[39m.\u001b[39mshuffle(patients_ids)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe truth value of a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is ambiguous. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# PREPARING DATA SPLITTING\n",
    "\n",
    "# get the set of patients id\n",
    "patients_ids = set(images_df[\"patient_id\"])\n",
    "patients_ids = list(patients_ids)\n",
    "\n",
    "# print the number of frames per score for the patient 1017 exam 1047 spot 1\n",
    "p = \"1050\"\n",
    "e = \"1127\"\n",
    "s = \"1\"\n",
    "print(images_df[(images_df[\"patient_id\"] == p) & (images_df[\"exam_id\"] == e) & (images_df[\"spot\"] == s)].groupby(\"score\").count())\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "random.shuffle(patients_ids)\n",
    "train_patients = patients_ids[:int(len(patients_ids)*0.7)]\n",
    "test_patients = patients_ids[int(len(patients_ids)*0.7):]\n",
    "print(\"train patients: \", train_patients, \"test patients: \", test_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "class MIDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, patient_list: list):\n",
    "        tmp = dataset[dataset[\"patient_id\"].isin(patient_list)]\n",
    "        # create a list with the path of the images\n",
    "        self.images_paths = [\"images/\" + \"_\".join(row) + \".png\" for row in tmp.values]\n",
    "        # self.images_paths to np array\n",
    "        self.images_paths = np.array(self.images_paths)\n",
    "        # create a list with the score of the images\n",
    "        self.images_scores = tmp[\"score\"].values\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.images_paths[idx])\n",
    "        label = self.images_scores[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Davide/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  ('0', '1', '2', '2', '3', '3', '3', '1', '3', '0', '2', '3', '1', '2', '3', '2', '3', '2', '1', '3', '3', '2', '3', '0', '1', '2', '3', '1', '0', '1', '1', '2')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 12.00 GiB total capacity; 10.81 GiB already allocated; 0 bytes free; 10.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [87], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m model \u001b[39m=\u001b[39m train(model, dataloader, dataloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m    133\u001b[0m \u001b[39m# define the test dataset\u001b[39;00m\n\u001b[0;32m    134\u001b[0m test_dataset \u001b[39m=\u001b[39m MIDataset(images_df, test_patients)\n",
      "Cell \u001b[1;32mIn [87], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput: \u001b[39m\u001b[39m\"\u001b[39m, output)\n\u001b[0;32m     41\u001b[0m \u001b[39m# calculate the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [87], line 15\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# from torch.cuda.ByteTensor to torch.cuda.FloatTensor\u001b[39;00m\n\u001b[0;32m     14\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> 15\u001b[0m resnet_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet18(x)\n\u001b[0;32m     16\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrink(resnet_res)\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 12.00 GiB total capacity; 10.81 GiB already allocated; 0 bytes free; 10.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# fine tune resnet18 to work with 4 output classes\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.resnet18 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "        self.shrink = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # set image back to 3 channels\n",
    "        x = torch.cat((x, x, x), 1)\n",
    "        # from torch.cuda.ByteTensor to torch.cuda.FloatTensor\n",
    "        x = x.float()\n",
    "        resnet_res = self.resnet18(x)\n",
    "        res = self.shrink(resnet_res)\n",
    "        return res\n",
    "\n",
    "# define the training loop\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    # set the best accuracy to 0\n",
    "    best_accuracy = 0\n",
    "    # loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # loop over the batches\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # move data and target to device\n",
    "            data = data.to(device)\n",
    "            print(\"target: \", target)\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            print(\"output: \", output)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            # print the loss\n",
    "            print(\"Epoch: \", epoch, \"Batch: \", batch_idx, \"Loss: \", loss.item())\n",
    "        # evaluate the model   \n",
    "        accuracy = evaluate(model, val_loader)\n",
    "        # if the accuracy is better than the best accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            # save the model\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            # update the best accuracy\n",
    "            best_accuracy = accuracy\n",
    "            print(\"Best accuracy: \", best_accuracy)\n",
    "    return model\n",
    "\n",
    "# define the evaluation loop\n",
    "def evaluate(model, val_loader):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    # set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    # set the number of correct predictions to 0\n",
    "    correct = 0\n",
    "    # set the number of total predictions to 0\n",
    "    total = 0\n",
    "    # loop over the batches\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        # move data and target to device\n",
    "        data = data.to(device)\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # get the predictions\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # update the total number of predictions\n",
    "        total += target.size(0)\n",
    "        # update the number of correct predictions\n",
    "        correct += (predicted == target).sum().item()\n",
    "    # calculate the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print the accuracy\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "# define the test loop\n",
    "def test(model, test_loader):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    # set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    # set the number of correct predictions to 0\n",
    "    correct = 0\n",
    "    # set the number of total predictions to 0\n",
    "    total = 0\n",
    "    # loop over the batches\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # move data and target to device\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # get the predictions\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # update the total number of predictions\n",
    "        total += target.size(0)\n",
    "        # update the number of correct predictions\n",
    "        correct += (predicted == target).sum().item()\n",
    "    # calculate the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    # print the accuracy\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "# define the dataset\n",
    "dataset = MIDataset(images_df, train_patients)\n",
    "# define the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# define the model\n",
    "model = MyModel()\n",
    "# define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define the optimizer SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# train the model\n",
    "model = train(model, dataloader, dataloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# define the test dataset\n",
    "test_dataset = MIDataset(images_df, test_patients)\n",
    "# define the test dataloader\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "# test the model\n",
    "result = test(model, test_dataloader)\n",
    "print(\"Test accuracy: \", result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_dataset = MIDataset(images_df, train_patients)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = MIDataset(images_df, test_patients)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def initialize_resnet18(num_classes = 4):\n",
    "    resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "    \n",
    "    in_features = resnet18.fc.in_features\n",
    "    \n",
    "    resnet18.fc = torch.nn.Linear(in_features=in_features,\n",
    "                                  out_features=num_classes)\n",
    "    return resnet18\n",
    "\n",
    "def get_optimizer(model, lr=0.001):\n",
    "    final_layer_weights = []\n",
    "    rest_of_the_net_weights = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('fc'):\n",
    "            final_layer_weights.append(param)\n",
    "        else:\n",
    "            rest_of_the_net_weights.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([\n",
    "            {'params': rest_of_the_net_weights},\n",
    "            {'params': final_layer_weights, 'lr': lr}\n",
    "        ], lr=lr)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(net, dataloader, optimizer, loss_function, device='cuda'):\n",
    "    \n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        print(inputs.shape)\n",
    "        print(targets.shape)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets[0].to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        \n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(net, dataloader, loss_function, device='cuda'):\n",
    "    \n",
    "    samples = 0.\n",
    "    cumulative_loss = 0.\n",
    "    cumulative_accuracy = 0.\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(dim=1)\n",
    "            \n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  torch.Size([1, 688, 880])\n",
      "label:  3\n",
      "image shape:  torch.Size([1, 688, 880])\n",
      "label:  1\n",
      "torch.Size([2, 1, 688, 880])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m get_optimizer(net, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):   \n\u001b[1;32m----> 5\u001b[0m         train_loss, train_accuracy \u001b[39m=\u001b[39m training_step(net, train_loader, optimizer, loss_function, device)\n\u001b[0;32m      6\u001b[0m         test_loss, test_accuracy \u001b[39m=\u001b[39m test_step(net, test_loader, loss_function, device)\n",
      "Cell \u001b[1;32mIn [15], line 11\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(net, dataloader, optimizer, loss_function, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mprint\u001b[39m(targets\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m     13\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     targets \u001b[39m=\u001b[39m targets[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "net = initialize_resnet18(4).to(device)\n",
    "optimizer = get_optimizer(net, lr=0.001)\n",
    "\n",
    "for e in range(20):   \n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, loss_function, device)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, loss_function, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "760bf3b9c43a19e2fe1b4d509841bd16027d034122dfd0e7f3a29ff3458619a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
