{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimilarityCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimilarityCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# create instance of the CNN\n",
    "cnn = SimilarityCNN()\n",
    "\n",
    "# extract features from an image\n",
    "features = cnn(image)\n",
    "\n",
    "# calculate cosine similarity between two feature vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "\n",
    "similarity = cosine_similarity(features1, features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify image with classification model\n",
    "output = model(image)\n",
    "_, predicted = torch.max(output, 1)\n",
    "confidence = output[0][predicted]\n",
    "\n",
    "# if confidence is below threshold, use similarity detection model\n",
    "if confidence < threshold:\n",
    "    features = cnn(image)\n",
    "    similarity = cosine_similarity(features, precious_features)\n",
    "    if similarity > similarity_threshold:\n",
    "        predicted = precious_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSH(nn.Module):\n",
    "    def __init__(self, num_hashes, hash_size):\n",
    "        super(LSH, self).__init__()\n",
    "        self.num_hashes = num_hashes\n",
    "        self.hash_size = hash_size\n",
    "        self.hashes = nn.ParameterList([nn.Parameter(torch.randn(hash_size)) for _ in range(num_hashes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate hash values for each hash function\n",
    "        hash_values = []\n",
    "        for h in self.hashes:\n",
    "            hash_values.append((x @ h).sign().long())\n",
    "        return torch.cat(hash_values, dim=1)\n",
    "\n",
    "# create LSH model with 10 hash functions and hash size 128\n",
    "lsh = LSH(10, 128)\n",
    "\n",
    "# generate random feature vectors to use as \"precious\" samples\n",
    "precious_features = torch.randn(100, 128)\n",
    "\n",
    "# calculate hash values for the precious samples\n",
    "precious_hashes = lsh(precious_features)\n",
    "\n",
    "# extract features from a new image\n",
    "features = cnn(image)\n",
    "\n",
    "# calculate hash values for the image\n",
    "hashes = lsh(features)\n",
    "\n",
    "# search for similar images by counting the number of hash functions that give the same value for both the image and a precious sample\n",
    "similarity = (precious_hashes == hashes).sum(dim=1).float() / lsh.num_hashes\n",
    "\n",
    "# find the most similar precious sample\n",
    "_, most_similar = similarity.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to store softmax values and labels for incorrect predictions\n",
    "incorrect_logits = []\n",
    "incorrect_labels = []\n",
    "\n",
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# turn off gradients for validation, saves memory and computations\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # get the predicted class from the maximum value in the output-probability tensor\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        # save softmax values and labels for incorrect predictions\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i] != target[i]:\n",
    "                incorrect_logits.append(output[i])\n",
    "                incorrect_labels.append(target[i])\n",
    "\n",
    "# convert lists to tensors\n",
    "incorrect_logits = torch.stack(incorrect_logits)\n",
    "incorrect_labels = torch.LongTensor(incorrect_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " incorrect_logits and incorrect_labels contain the softmax values and labels, respectively, for all the incorrect predictions made by the model on the validation set. You can use these tensors to analyze the behavior of the model when it makes incorrect predictions.\n",
    "\n",
    " For example, you could calculate the average softmax values for each class and compare them to see which classes the model is most confident about when it makes incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average softmax values for each class\n",
    "avg_logits = incorrect_logits.mean(dim=0)\n",
    "\n",
    "# print average softmax values\n",
    "for i in range(len(avg_logits)):\n",
    "    print('Class {}: {:.3f}'.format(i, avg_logits[i]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
